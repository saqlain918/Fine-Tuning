{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "import gradio as gr\n",
        "\n",
        "data = [\n",
        "    {\"text\": \"Riddle: What number becomes zero when you subtract 15 from half of it?\\nAnswer: 30\"},\n",
        "    {\"text\": \"Riddle: I am a number that when doubled and then reduced by 20 gives 40.\\nAnswer: 30\"},\n",
        "    {\"text\": \"Riddle: If you add 10 to a number and then subtract 5, you get 25.\\nAnswer: 20\"},\n",
        "    {\"text\": \"Riddle: I am 15 less than twice my value.\\nAnswer: 15\"},\n",
        "    {\"text\": \"Riddle: A number when halved and then increased by 10 becomes 25.\\nAnswer: 30\"},\n",
        "    {\"text\": \"Riddle: When you multiply a number by 3 and subtract 9, the result is 18.\\nAnswer: 9\"},\n",
        "    {\"text\": \"Riddle: If a number is decreased by 8 and then doubled, you get 14.\\nAnswer: 15\"},\n",
        "    {\"text\": \"Riddle: A number when tripled and then increased by 5 equals 20.\\nAnswer: 5\"},\n",
        "    {\"text\": \"Riddle: When you add 7 to half of a number, you get 19.\\nAnswer: 24\"},\n",
        "    {\"text\": \"Riddle: A number is increased by 9 and then halved to get 15.\\nAnswer: 21\"},\n",
        "    {\"text\": \"Riddle: When you subtract 4 from a number and then multiply by 3, the result is 33.\\nAnswer: 15\"},\n",
        "    {\"text\": \"Riddle: A number reduced by 6 equals one-third of itself.\\nAnswer: 9\"},\n",
        "    {\"text\": \"Riddle: When you double a number and add 10, you get 30.\\nAnswer: 10\"},\n",
        "    {\"text\": \"Riddle: A number, when 5 is subtracted and then multiplied by 2, gives 20.\\nAnswer: 15\"},\n",
        "    {\"text\": \"Riddle: If a number is multiplied by 4 and then decreased by 8, the result is 24.\\nAnswer: 8\"},\n",
        "    {\"text\": \"Riddle: A number, when divided by 2 and then increased by 7, equals 17.\\nAnswer: 20\"},\n",
        "    {\"text\": \"Riddle: When you subtract 3 from a number and then square the result, you get 49.\\nAnswer: 10\"},\n",
        "    {\"text\": \"Riddle: If 12 is added to a number, the result is three times the number.\\nAnswer: 6\"},\n",
        "    {\"text\": \"Riddle: A number increased by 50% equals 27.\\nAnswer: 18\"},\n",
        "    {\"text\": \"Riddle: If a number is halved and then 4 is subtracted, the result is 8.\\nAnswer: 24\"},\n",
        "    {\"text\": \"Riddle: A number, when 2 is added, becomes twice the original number.\\nAnswer: 2\"},\n",
        "    {\"text\": \"Riddle: When you triple a number and subtract 7, the result is 14.\\nAnswer: 7\"},\n",
        "    {\"text\": \"Riddle: A number, when reduced by 2 and then divided by 4, gives 5.\\nAnswer: 22\"},\n",
        "    {\"text\": \"Riddle: When you add 8 to a number and then multiply by 2, you get 40.\\nAnswer: 12\"},\n",
        "    {\"text\": \"Riddle: A number, when doubled, is 16 more than the number itself.\\nAnswer: 16\"},\n",
        "    {\"text\": \"Riddle: A number that is increased by 3 and then multiplied by 2 equals 26.\\nAnswer: 10\"},\n",
        "    {\"text\": \"Riddle: A number when reduced by 4 and then doubled equals 12.\\nAnswer: 10\"},\n",
        "    {\"text\": \"Riddle: If you subtract 2 from a number and then double it, you get 14.\\nAnswer: 9\"},\n",
        "    {\"text\": \"Riddle: A number when tripled and decreased by 5 equals 16.\\nAnswer: 7\"},\n",
        "    {\"text\": \"Riddle: If you add 5 to a number and then double the result, you get 30.\\nAnswer: 10\"}\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])  # remove raw text column if not needed\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))  # Adjust for the added pad token\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-math-riddle\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,                    # Increased epochs\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,          # Simulate a larger batch size\n",
        "    learning_rate=3e-5,                     # Lower learning rate\n",
        "    weight_decay=0.01,                      # Optional: add weight decay\n",
        "    warmup_steps=100,                       # Optional: add warmup steps\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    prediction_loss_only=True,\n",
        "    report_to=[]                           # Disable wandb logging\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Update model config for pad_token_id if not already set\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Gradio UI for testing\n",
        "def generate_riddle(prompt):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=35,             # Further limit the output length\n",
        "        do_sample=True,            # Enable sampling\n",
        "        top_k=50,                  # Top-k sampling\n",
        "        top_p=0.92,                # Nucleus sampling\n",
        "        temperature=0.5,           # Lower temperature for more deterministic outputs\n",
        "        repetition_penalty=1.2,    # Penalize repetition\n",
        "        no_repeat_ngram_size=3,    # Prevent 3-gram repetition\n",
        "        num_return_sequences=5,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_texts = []\n",
        "    for output in outputs:\n",
        "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        if \"\\nAnswer:\" in generated_text:\n",
        "            parts = generated_text.split(\"\\nAnswer:\")\n",
        "            answer_part = parts[1].split('.')[0] + \".\"\n",
        "            generated_text = parts[0] + \"\\nAnswer:\" + answer_part\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "iface = gr.Interface(fn=generate_riddle, inputs=\"text\", outputs=\"text\", title=\"Math Riddle Generator\", description=\"Enter a prompt to generate a riddle.\")\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "fak5DDux96hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "import gradio as gr  # Import Gradio for UI\n",
        "\n",
        "# Define a dataset of 20 incorrect math memes with corrections and explanations.\n",
        "data = [\n",
        "    {\"text\": \"Incorrect: 8 ÷ 2(2+2) = 1? Correct: 8 ÷ 2(2+2) = 16. Explanation: Evaluate parentheses first then perform division and multiplication sequentially.\"},\n",
        "    {\"text\": \"Incorrect: 5 + 5 = 20? Correct: 5 + 5 = 10. Explanation: Simple addition error.\"},\n",
        "    {\"text\": \"Incorrect: 6 * 6 = 36 but 6 / 6 = 6? Correct: 6 / 6 = 1. Explanation: A number divided by itself equals 1.\"},\n",
        "    {\"text\": \"Incorrect: 2^3 = 6? Correct: 2^3 = 8. Explanation: 2 cubed is 8.\"},\n",
        "    {\"text\": \"Incorrect: √16 = 5? Correct: √16 = 4. Explanation: The square root of 16 is 4.\"},\n",
        "    {\"text\": \"Incorrect: 9 - 3 = 3? Correct: 9 - 3 = 6. Explanation: Correct subtraction yields 6.\"},\n",
        "    {\"text\": \"Incorrect: 4 * 4 = 8? Correct: 4 * 4 = 16. Explanation: Multiplication error.\"},\n",
        "    {\"text\": \"Incorrect: 10 / 2 = 10? Correct: 10 / 2 = 5. Explanation: Division error.\"},\n",
        "    {\"text\": \"Incorrect: 15% of 200 = 50? Correct: 15% of 200 = 30. Explanation: 15% of 200 equals 30.\"},\n",
        "    {\"text\": \"Incorrect: 100 / 4 = 20? Correct: 100 / 4 = 25. Explanation: Division error.\"},\n",
        "    {\"text\": \"Incorrect: 3 + 7 = 11? Correct: 3 + 7 = 10. Explanation: 3 plus 7 equals 10.\"},\n",
        "    {\"text\": \"Incorrect: 2 * 3 + 4 = 14? Correct: 2 * 3 + 4 = 10. Explanation: Follow order of operations: multiply then add.\"},\n",
        "    {\"text\": \"Incorrect: 12 / 3 * 2 = 10? Correct: 12 / 3 * 2 = 8. Explanation: 12 divided by 3 is 4; 4 times 2 is 8.\"},\n",
        "    {\"text\": \"Incorrect: 7 * 7 = 42? Correct: 7 * 7 = 49. Explanation: Multiplication error.\"},\n",
        "    {\"text\": \"Incorrect: 14 - 7 = 8? Correct: 14 - 7 = 7. Explanation: Subtraction error.\"},\n",
        "    {\"text\": \"Incorrect: (3 + 2) * 2 = 12? Correct: (3 + 2) * 2 = 10. Explanation: Add first, then multiply.\"},\n",
        "    {\"text\": \"Incorrect: 50% of 100 = 60? Correct: 50% of 100 = 50. Explanation: 50% is half of 100.\"},\n",
        "    {\"text\": \"Incorrect: 9 + 9 = 18 then 18 / 2 = 10? Correct: 18 / 2 = 9. Explanation: Division error.\"},\n",
        "    {\"text\": \"Incorrect: 5! = 100? Correct: 5! = 120. Explanation: 5 factorial is 120.\"},\n",
        "    {\"text\": \"Incorrect: 3^2 + 4^2 = 14? Correct: 3^2 + 4^2 = 25. Explanation: 9 + 16 equals 25.\"}\n",
        "]\n",
        "\n",
        "# Convert the list to a Hugging Face Dataset.\n",
        "dataset = Dataset.from_list(data)\n",
        "print(\"Dataset created with\", len(dataset), \"examples.\")\n",
        "\n",
        "# Load the GPT-2 tokenizer and model.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT-2 does not have an official pad token; use the eos_token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Configure LoRA for efficient fine-tuning.\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",  # For language modeling.\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA.\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Model loaded and LoRA configured.\")\n",
        "\n",
        "# Tokenize each example.\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "print(\"Dataset tokenized.\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=5,  # Increase epochs to help the model learn from 20 examples.\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,  # Slightly lower learning rate.\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Gradio UI for testing the model\n",
        "def correct_math(prompt):\n",
        "    model.eval()  # Set model to evaluation mode.\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.90,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return result\n",
        "\n",
        "# Create Gradio interface\n",
        "gr.Interface(fn=correct_math, inputs=\"text\", outputs=\"text\", title=\"Math Correction Model\", description=\"Enter an incorrect math statement to get the correct answer and explanation.\").launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "8MQ61DUb96nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling, GPT2Tokenizer\n",
        "import gradio as gr\n",
        "\n",
        "# Create a list of 30 emoji math problems with their solutions.\n",
        "# Format: \"Q: [emoji math equation]\\nA: [solution]\"\n",
        "data = [\n",
        "    \"Q: 🍎 + 🍎 + 🍎 = 12\\nA: 4\",\n",
        "    \"Q: 🎲 + 🎲 = 12\\nA: 6\",\n",
        "    \"Q: 🚗 + 🚗 + 🚗 + 🚗 = 20\\nA: 5\",\n",
        "    \"Q: 🍌 + 🍌 + 🍌 + 🍌 + 🍌 = 15\\nA: 3\",\n",
        "    \"Q: 🍓 + 🍓 = 8\\nA: 4\",\n",
        "    \"Q: 🍕 + 🍕 + 🍕 = 18\\nA: 6\",\n",
        "    \"Q: 🍩 + 🍩 + 🍩 + 🍩 = 20\\nA: 5\",\n",
        "    \"Q: 🌟 + 🌟 + 🌟 = 9\\nA: 3\",\n",
        "    \"Q: 🎈 + 🎈 = 14\\nA: 7\",\n",
        "    \"Q: 🎂 + 🎂 + 🎂 = 15\\nA: 5\",\n",
        "    \"Q: 🍪 + 🍪 + 🍪 + 🍪 = 16\\nA: 4\",\n",
        "    \"Q: 🍭 + 🍭 + 🍭 = 15\\nA: 5\",\n",
        "    \"Q: 🧁 + 🧁 = 10\\nA: 5\",\n",
        "    \"Q: 🥑 + 🥑 + 🥑 = 12\\nA: 4\",\n",
        "    \"Q: 🍇 + 🍇 = 10\\nA: 5\",\n",
        "    \"Q: 🍒 + 🍒 + 🍒 = 15\\nA: 5\",\n",
        "    \"Q: 🍍 + 🍍 = 14\\nA: 7\",\n",
        "    \"Q: 🍉 + 🍉 + 🍉 + 🍉 = 20\\nA: 5\",\n",
        "    \"Q: 🥭 + 🥭 = 16\\nA: 8\",\n",
        "    \"Q: 🍈 + 🍈 + 🍈 = 9\\nA: 3\",\n",
        "    \"Q: 🍑 + 🍑 + 🍑 + 🍑 = 20\\nA: 5\",\n",
        "    \"Q: 🍏 + 🍏 = 10\\nA: 5\",\n",
        "    \"Q: 🍋 + 🍋 + 🍋 = 12\\nA: 4\",\n",
        "    \"Q: 🍊 + 🍊 = 10\\nA: 5\",\n",
        "    \"Q: 🥝 + 🥝 + 🥝 = 15\\nA: 5\",\n",
        "    \"Q: 🍐 + 🍐 = 8\\nA: 4\",\n",
        "    \"Q: 🍆 + 🍆 + 🍆 + 🍆 = 16\\nA: 4\",\n",
        "    \"Q: 🥕 + 🥕 = 10\\nA: 5\",\n",
        "    \"Q: 🌽 + 🌽 + 🌽 = 9\\nA: 3\",\n",
        "    \"Q: 🥔 + 🥔 + 🥔 + 🥔 = 20\\nA: 5\"\n",
        "]\n",
        "\n",
        "# For training with Hugging Face's datasets, we create a dictionary.\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict({\"text\": data})\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# Load GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Create a data collator for language modeling that will handle padding dynamically\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./emoji_math_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    learning_rate=1e-5\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Suppress transformer warnings\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_single_answer(prompt, max_new_tokens=10):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=2.0\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    if prompt in generated_text:\n",
        "        generated_text = generated_text.split(prompt, 1)[1]\n",
        "\n",
        "    match = re.search(r'\\b(\\d+)\\b', generated_text)\n",
        "    if match:\n",
        "        answer = match.group(1)\n",
        "    else:\n",
        "        answer = generated_text.strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Gradio UI\n",
        "def gradio_interface(prompt):\n",
        "    return generate_single_answer(prompt)\n",
        "\n",
        "iface = gr.Interface(fn=gradio_interface, inputs=\"text\", outputs=\"text\", title=\"Emoji Math Solver\", description=\"Enter an emoji math problem to get the answer.\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "QcyFQVoU964P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}